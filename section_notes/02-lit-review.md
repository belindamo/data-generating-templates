

# Literature Review

## Key Papers

### Cookbook: A Framework for Improving LLM Generative Abilities via Programmatic Data Generating Templates (Narayan et al., 2024)

* **Contribution:** Introduces programmatic data generation using simple patterns over random tokens, achieving up to 52.7 accuracy point improvements while avoiding privacy and legal issues of LLM-generated data
* **Assumption:** Training data must be semantically meaningful and generated by humans or LLMs  
* **Gap:** Limited to pattern-based rules; unclear extension to complex reasoning tasks

### TemplateMath: Training and Evaluating Language Models with Template-based Data Generation (Zhang et al., 2024)

* **Contribution:** Leverages LLMs to generate parameterized meta-templates for synthesizing 7M+ mathematical problems, addressing dataset scarcity in mathematical reasoning
* **Assumption:** Mathematical datasets must be manually created by experts with limited scope
* **Gap:** Domain-specific to mathematics; dependency on GPT-4 for meta-template generation

### SynAlign: Few-shot LLM Synthetic Data with Distribution Matching (Ren et al., 2025)

* **Contribution:** Addresses distribution mismatch between synthetic and real data through uncertainty tracking and Maximum Mean Discrepancy-based sampling weight learning
* **Assumption:** Synthetic data can be directly mixed with real data without distribution considerations
* **Gap:** Requires sophisticated distribution matching techniques; may not generalize across all data types

### BARE: Base-Refine for Few-Shot Synthetic Data Generation (Zhu et al., 2025)

* **Contribution:** Demonstrates that base models (without post-training) offer superior diversity for synthetic data generation compared to instruction-tuned models, requiring only 3 seed examples
* **Assumption:** Instruction-tuned models are superior for all synthetic data generation tasks
* **Gap:** Two-stage process adds complexity; performance depends on quality of refinement stage

### GLAN: Generalized Instruction Tuning (Li et al., 2024)

* **Contribution:** Uses systematic taxonomy of human knowledge to generate comprehensive instruction data across all disciplines, eliminating need for seed examples
* **Assumption:** Instruction datasets must be constructed from existing examples or task-specific data
* **Gap:** Requires extensive taxonomy development; may miss domain-specific nuances

### TarGEN: Targeted Data Generation (Gupta et al., 2023)

* **Contribution:** Multi-step prompting strategy with self-correction capabilities for seedless data generation, achieving 1-2% improvements on SuperGLUE
* **Assumption:** Synthetic data generation requires specific task instances as seeds
* **Gap:** Self-correction mechanism may not catch all generation errors; limited diversity exploration

### AgentIF: Benchmarking Instruction Following in Agentic Scenarios (Qi et al., 2025)

* **Contribution:** First systematic benchmark for evaluating instruction following in realistic agentic scenarios with long, complex instructions (avg. 1,723 words, 11.9 constraints)
* **Assumption:** Standard instruction following evaluations are sufficient for agentic applications
* **Gap:** Reveals poor performance of current LLMs on complex instructions but limited to evaluation

### SIFo: Sequential Instruction Following Benchmark (Chen et al., 2024)

* **Contribution:** Addresses coherence, positional bias, and verification challenges in multi-instruction evaluation through sequential task design
* **Assumption:** Single instruction evaluation captures multi-instruction capabilities
* **Gap:** All models struggle with instruction sequences, indicating fundamental robustness limitations

### WildChat-50m: Role of Synthetic Data in Post-Training (Feuer et al., 2025)

* **Contribution:** Largest public chat dataset (50m conversations) enabling comparative analysis of synthetic data generating models, creating Re-Wild SFT mix that outperforms Tulu-3 with 40% fewer samples
* **Assumption:** Large-scale comparative analysis of synthetic data quality is intractable
* **Gap:** Focuses primarily on chat data; may not generalize to other domains

### MaXIFE: Multilingual Instruction Following Evaluation (Liu et al., 2025)

* **Contribution:** Comprehensive multilingual instruction following benchmark across 23 languages with 1,667 verifiable tasks, revealing 25-35% accuracy gaps between high/low-resource languages
* **Assumption:** English-centric evaluation adequately captures instruction following capabilities
* **Gap:** Evaluation-focused; doesn't address how to improve multilingual instruction following

## Common Assumptions Across Literature

1. **Training Data Quality Requirements**: Most work assumes effective training data must be either human-generated (expensive, limited) or LLM-generated (privacy issues). The Cookbook framework challenges this by showing programmatic generation over random tokens can be effective.

2. **Semantic Meaningfulness Necessity**: Traditional approaches assume training data must be semantically meaningful. However, pattern-based approaches (Cookbook, TemplateMath) demonstrate that structural patterns can transfer to meaningful tasks.

3. **Scale Requirements for Synthesis**: Many methods assume large seed datasets are necessary for quality synthetic generation. BARE and TarGEN challenge this by achieving good results with minimal seeds (3 examples for BARE).

4. **Distribution Ignorance in Mixing**: Prior work often assumes synthetic data can be directly combined with real data. SynAlign demonstrates this causes distribution distortion and proposes explicit distribution matching.

5. **Instruction Tuning Supremacy**: Common assumption that instruction-tuned models are superior for all generation tasks. BARE shows base models provide better diversity for synthetic data generation.

6. **Single-Language/Task Evaluation Sufficiency**: Most evaluation assumes English-centric or single-task evaluation captures model capabilities. MaXIFE and AgentIF reveal significant gaps in multilingual and complex scenarios.

7. **Template Limitations to Simple Patterns**: Assumption that template-based approaches only work for simple, rule-based tasks. Recent work (GLAN, TemplateMath) shows broader applicability across reasoning domains.

## Our Position

### Challenges
We question the assumption that **pattern-based data generation is limited to simple tasks**. While Cookbook shows effectiveness for basic instruction following, we hypothesize that more sophisticated template structures can capture complex reasoning patterns and domain-specific knowledge.

### Builds On
Our work extends the **Cookbook framework** in several key directions:
1. **Beyond Random Tokens**: Investigating structured content generation within template frameworks
2. **Complex Reasoning Patterns**: Developing templates that capture multi-step reasoning, causal relationships, and domain expertise
3. **Scalable Template Discovery**: Automated methods for discovering effective template patterns from minimal examples
4. **Cross-Domain Generalization**: Understanding how templates learned in one domain transfer to related domains

We also build on **TemplateMath's meta-template concept** by exploring how to automatically discover and refine templates without requiring large language models like GPT-4, making the approach more accessible and cost-effective.

The **distribution matching insights from SynAlign** inform our approach to ensuring generated data complements rather than distorts existing training distributions, while **BARE's diversity findings** guide our template design to maximize coverage of the solution space.

